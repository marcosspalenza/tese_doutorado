%\pagestyle{empty}
%\cleardoublepage
%\pagestyle{fancy}
%\pagenumbering{arabic}

\section{Trabalhos Relacionados}\label{cap2}

Segundo \citeonline{perez-marin2009}, apoiando-se na evolução das técnicas de inteligência computacional, a Avaliação Assistida por Computadores evoluiu muito na última década. Estudos passaram da análise simples do modelo de gradação da nota para interpretação do conteúdo e da metodologia como um todo. Então, a partir dessa essa evolução, os sistema começaram a reconhecer padrões para modelar o critério avaliativo do professor nas atividades.

O estudo realizado por \citeonline{butcher2010} é resultado da comparação entre os sumários elaborados por especialistas e métodos automáticos baseados em linguística computacional ou em extração de palavras-chave. Para estabelecer essa comparação foram utilizados seis especialistas para avaliarem de forma binária (correto / incorreto) cada questão. Os sistemas comparados foram o \textit{Intelligent Assessment Technologies FreeText Author} para linguística computacional, o \textit{OpenMark} e expressões regulares para extração de palavras-chave. 

Segundo essa pesquisa o sistema avaliado, o \textit{FreeText Author}, têm resultados equivalentes à análise dos especialistas. Quando comparado com os outros sistemas, há uma proximidade entre resultados. A conformidade da ferramenta com os especialistas foi acima de 90\% de acurácia na marcação, com desempenho superior aos das ferramentas de extração de palavras-chave. A abordagem porém só leva em consideração níveis de classe binários, sendo a avaliação por classe de nota um problema mais complexo.

Voltado diretamente para verificação do sistema automático para avaliação de respostas discursivas curtas, \citeonline{mohler2011} abordam o tema com análise pela similaridade das respostas com apoio de grafos de dependência textual. A proposta deste artigo, portanto, foi a extração do contexto dos termos para melhoria do processo de avaliação. A resposta candidata passada pelo especialista foi utilizada como base para extração de características e reconhecimento de padrões de escrita através da similaridade dos documentos e da interpretação sistêmica dos grafos de dependência morfossintática. A comparação direta com a resposta candidata, pré-estabelecida como correta, foi realizada segundo oito medidas de similaridade semântica. A associação dos dois módulos, a análise documental por similaridade e o grafo de dependências, gera o padrão de correção do sistema.

No trabalho a base de dados utilizada para os testes foi coletada na Universidade do Norte do Texas na disciplina de Estrutura de Dados. O \textit{Texas Dataset}, como foi chamado pelos autores, é descrito com mais detalhes na Seção \ref{cap5}.  Nesse \textit{dataset}, os avaliadores humanos coincidiram na correção de 0 a 5 pontos em apenas 57,7\% das notas e em 80,6\% se fossem desconsiderados erros de até 1 ponto de divergência na avaliação. Os resultados obtidos na classificação pelo sistema foram de 85\% de \textit{precision} e 62\% de \textit{recall}, estratificado em \textit{12-fold cross validation}. A raiz do erro médio quadrático apresentado foi de 0,98 pontos, sendo de 0,86 pontos quando computado por questão. A correlação alcançada entre as respostas para essa base de dados foi de 0,480.

A metodologia proposta por \citeonline{gomaa2012} aplica análise de similaridade de texto para desenvolver avaliações mais próximas das respostas candidatas. Aplicado ao \textit{Texas Dataset}, o sistema utiliza métricas complementares por termos e por \textit{corpus}. Na primeira metodologia, de análise morfológica por termos, foram 13 métodos distintos aplicados entre a comparação de caracteres e as métricas de similaridade. Na segunda foram dois métodos: distribuição de palavras similares e co-ocorrência (DISCO), computando a frequência \textit{I) ordenada pela similaridade sobre duas palavras em colocação (correferência) no corpus e II) ordenada pela similaridade sobre duas palavras em termos similares}. De forma não supervisionada, este estudo alcançou resultados de correlação de 0,504 acima do \textit{baseline} \cite{mohler2011}, identificando maior tendência nas respostas das questões.

Um algoritmo distinto para o processo de comparação com respostas candidatas é resultado do estudo realizado por \citeonline{noorbehbahani2011}. Neste artigo a resposta apresentada pelo professor é comparada com o \textit{dataset} através do módulo de avaliação de sumários \textit{ROUGE}, do método de extração de contexto \textit{Latent Semantic Analysis - LSA}, do algoritmo de comparação entre respostas \textit{ERB}, do sistema de avaliação de traduções \textit{BLEU} e do cálculo de co-ocorrência de \textit{n-grams}. Foram adicionados ainda uma modificação do algoritmo \textit{BLEU} para abordagens de correção para respostas discursivas e uma ponderação entre \textit{n-grams} e o LSA. O \textit{dataset} foi construído através de um AVA, coletando 45 questões e 300 respostas. Como resultado o artigo apresentou a covariância entre os métodos e a resposta candidata, sendo o melhor desempenho o do \textit{BLEU} modificado com 0,85. A correlação medida foi dada pela covariância entre a similaridade das respostas e a nota do especialista pelo desvio padrão de ambos.

No artigo de \citeonline{ramachandran2015a}, os autores mostram uma abordagem relacionada com a análise dos documentos de resposta dos alunos. O objetivo deste estudo foi criar de forma sistêmica a análise das respostas contornando a rigidez de uma única chave de correção. Neste trabalho os autores apresentam modelos criados a partir da sumarização das respostas bem avaliadas. Assim os sumários de classe são aguardados como correlatos ao modelo avaliativo do professor para a nota máxima. O método utilizado é a coesão por análise de grafos, similaridade e clusterização. As respostas mais representativas são extraídas conforme maior seja sua cobertura semântica \textit{intracluster}.

Em um trabalho complementar, \citeonline{ramachandran2015b} propõe uma forma de geração de respostas candidatas analisando os documentos avaliadas com nota máxima, o quadro de \textit{rubrics} e, se existir, o material de apoio, O modelo extrai relações entre conceitos do conteúdo retornando um conjunto de padrões de forma não ordenada. Os testes realizados mostram resultados equiparáveis com os do vencedor do concurso do \textit{Automatic Short Answer Grading - ASAP } do \textit{Kaggle}\footnote{Kaggle \url{https://www.kaggle.com}} substituindo as expressões regulares lá criadas manualmente. Outra base de dados testada, o \textit{Texas Dataset}, a raiz do erro médio quadrático foi reduzida de 0,98 para 0,86 pontos e de 0,86 para 0,77 pontos na análise por questão.

Outra técnica similar para análise do conteúdo em documentos dos alunos foi proposto por \citeonline{oliveira2013}. O objetivo deste é entregar ao estudante recomendações de classes de atividades como \textit{feedback} conforme o exercício submetido. Três bases de dados foram utilizadas, duas jornalísticas e uma de exercícios de programação. De acordo com o conhecimento do especialista na classificação/avaliação dos dados, a ferramenta enviou possíveis melhorias no aprendizado segundo o conteúdo. Os resultados de \textit{precision} apresentados com para tais dados foram de 89\%.